{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_id = 'retrained_sherlock'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier \n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2022-02-02 08:57:12.713750\n",
      "Load data (train) process took 0:00:03.294787 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(f'Started at {start}')\n",
    "\n",
    "X_train = pd.read_parquet('train.parquet')\n",
    "y_train = pd.read_parquet('../data/raw/train_labels.parquet').values.flatten()\n",
    "\n",
    "print(f'Load data (train) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct types for columns in the Dataframe (should be all float32):\n",
      "{dtype('float32')}\n"
     ]
    }
   ],
   "source": [
    "print('Distinct types for columns in the Dataframe (should be all float32):')\n",
    "print(set(X_train.dtypes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2022-02-02 08:57:16.192381\n",
      "Load data (validation) process took 0:00:01.407590 seconds.\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(f'Started at {start}')\n",
    "\n",
    "X_validation = pd.read_parquet('validation.parquet')\n",
    "y_validation = pd.read_parquet('../data/raw/val_labels.parquet').values.flatten()\n",
    "\n",
    "y_validation = np.array([x.lower() for x in y_validation])\n",
    "\n",
    "print(f'Load data (validation) process took {datetime.now() - start} seconds.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train, X_validation], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([x.lower() for x in itertools.chain(y_train, y_validation)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2022-02-02 08:57:19.883704\n",
      "Trained and saved new model.\n",
      "Finished at 2022-02-02 09:13:27.313795, took 0:16:07.430228 seconds\n"
     ]
    }
   ],
   "source": [
    "# n_estimators=300 gives a slightly better result (0.1%), but triples the fit time\n",
    "n_estimators=100\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1)\n",
    "et_clf = ExtraTreesClassifier(n_estimators=n_estimators, n_jobs=-1)\n",
    "\n",
    "start = datetime.now()\n",
    "print(f'Started at {start}')\n",
    "\n",
    "voting_clf = VotingClassifier(estimators=[('rd', rnd_clf), ('et', et_clf)], voting='soft')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "print('Trained and saved new model.')\n",
    "print(f'Finished at {datetime.now()}, took {datetime.now() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started at 2022-02-02 09:13:27.393033\n",
      "Trained and saved new model.\n",
      "Finished at 2022-02-02 09:13:29.390839, took 0:00:01.997820 seconds\n"
     ]
    }
   ],
   "source": [
    "start = datetime.now()\n",
    "print(f'Started at {start}')\n",
    "\n",
    "X_test = pd.read_parquet('test.parquet')\n",
    "y_test = pd.read_parquet('../data/raw/test_labels.parquet').values.flatten()\n",
    "\n",
    "y_test = np.array([x.lower() for x in y_test])\n",
    "\n",
    "print('Trained and saved new model.')\n",
    "print(f'Finished at {datetime.now()}, took {datetime.now() - start} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_prob = voting_clf.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.load(\n",
    "        f\"../sherlock/deploy/classes_{nn_model_id}.npy\",\n",
    "        allow_pickle=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (classes == sorted(classes)).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0202 09:13:55.804179 4680068608 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project-1/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0202 09:13:55.805443 4680068608 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project-1/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Ones.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0202 09:13:55.809165 4680068608 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project-1/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0202 09:13:55.828798 4680068608 deprecation.py:506] From /Users/lowecg/source/private-github/sherlock-project-1/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2022-02-02 09:13:56.323039: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "2022-02-02 09:13:56.345884: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f89babf1540 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-02-02 09:13:56.345897: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature cols: ['n_[0]-agg-any', 'n_[0]-agg-all', 'n_[0]-agg-mean', 'n_[0]-agg-var', 'n_[0]-agg-min', 'n_[0]-agg-max', 'n_[0]-agg-median', 'n_[0]-agg-sum', 'n_[0]-agg-kurtosis', 'n_[0]-agg-skewness', 'n_[1]-agg-any', 'n_[1]-agg-all', 'n_[1]-agg-mean', 'n_[1]-agg-var', 'n_[1]-agg-min', 'n_[1]-agg-max', 'n_[1]-agg-median', 'n_[1]-agg-sum', 'n_[1]-agg-kurtosis', 'n_[1]-agg-skewness', 'n_[2]-agg-any', 'n_[2]-agg-all', 'n_[2]-agg-mean', 'n_[2]-agg-var', 'n_[2]-agg-min', 'n_[2]-agg-max', 'n_[2]-agg-median', 'n_[2]-agg-sum', 'n_[2]-agg-kurtosis', 'n_[2]-agg-skewness', 'n_[3]-agg-any', 'n_[3]-agg-all', 'n_[3]-agg-mean', 'n_[3]-agg-var', 'n_[3]-agg-min', 'n_[3]-agg-max', 'n_[3]-agg-median', 'n_[3]-agg-sum', 'n_[3]-agg-kurtosis', 'n_[3]-agg-skewness', 'n_[4]-agg-any', 'n_[4]-agg-all', 'n_[4]-agg-mean', 'n_[4]-agg-var', 'n_[4]-agg-min', 'n_[4]-agg-max', 'n_[4]-agg-median', 'n_[4]-agg-sum', 'n_[4]-agg-kurtosis', 'n_[4]-agg-skewness', 'n_[5]-agg-any', 'n_[5]-agg-all', 'n_[5]-agg-mean', 'n_[5]-agg-var', 'n_[5]-agg-min', 'n_[5]-agg-max', 'n_[5]-agg-median', 'n_[5]-agg-sum', 'n_[5]-agg-kurtosis', 'n_[5]-agg-skewness', 'n_[6]-agg-any', 'n_[6]-agg-all', 'n_[6]-agg-mean', 'n_[6]-agg-var', 'n_[6]-agg-min', 'n_[6]-agg-max', 'n_[6]-agg-median', 'n_[6]-agg-sum', 'n_[6]-agg-kurtosis', 'n_[6]-agg-skewness', 'n_[7]-agg-any', 'n_[7]-agg-all', 'n_[7]-agg-mean', 'n_[7]-agg-var', 'n_[7]-agg-min', 'n_[7]-agg-max', 'n_[7]-agg-median', 'n_[7]-agg-sum', 'n_[7]-agg-kurtosis', 'n_[7]-agg-skewness', 'n_[8]-agg-any', 'n_[8]-agg-all', 'n_[8]-agg-mean', 'n_[8]-agg-var', 'n_[8]-agg-min', 'n_[8]-agg-max', 'n_[8]-agg-median', 'n_[8]-agg-sum', 'n_[8]-agg-kurtosis', 'n_[8]-agg-skewness', 'n_[9]-agg-any', 'n_[9]-agg-all', 'n_[9]-agg-mean', 'n_[9]-agg-var', 'n_[9]-agg-min', 'n_[9]-agg-max', 'n_[9]-agg-median', 'n_[9]-agg-sum', 'n_[9]-agg-kurtosis', 'n_[9]-agg-skewness', 'n_[a]-agg-any', 'n_[a]-agg-all', 'n_[a]-agg-mean', 'n_[a]-agg-var', 'n_[a]-agg-min', 'n_[a]-agg-max', 'n_[a]-agg-median', 'n_[a]-agg-sum', 'n_[a]-agg-kurtosis', 'n_[a]-agg-skewness', 'n_[b]-agg-any', 'n_[b]-agg-all', 'n_[b]-agg-mean', 'n_[b]-agg-var', 'n_[b]-agg-min', 'n_[b]-agg-max', 'n_[b]-agg-median', 'n_[b]-agg-sum', 'n_[b]-agg-kurtosis', 'n_[b]-agg-skewness', 'n_[c]-agg-any', 'n_[c]-agg-all', 'n_[c]-agg-mean', 'n_[c]-agg-var', 'n_[c]-agg-min', 'n_[c]-agg-max', 'n_[c]-agg-median', 'n_[c]-agg-sum', 'n_[c]-agg-kurtosis', 'n_[c]-agg-skewness', 'n_[d]-agg-any', 'n_[d]-agg-all', 'n_[d]-agg-mean', 'n_[d]-agg-var', 'n_[d]-agg-min', 'n_[d]-agg-max', 'n_[d]-agg-median', 'n_[d]-agg-sum', 'n_[d]-agg-kurtosis', 'n_[d]-agg-skewness', 'n_[e]-agg-any', 'n_[e]-agg-all', 'n_[e]-agg-mean', 'n_[e]-agg-var', 'n_[e]-agg-min', 'n_[e]-agg-max', 'n_[e]-agg-median', 'n_[e]-agg-sum', 'n_[e]-agg-kurtosis', 'n_[e]-agg-skewness', 'n_[f]-agg-any', 'n_[f]-agg-all', 'n_[f]-agg-mean', 'n_[f]-agg-var', 'n_[f]-agg-min', 'n_[f]-agg-max', 'n_[f]-agg-median', 'n_[f]-agg-sum', 'n_[f]-agg-kurtosis', 'n_[f]-agg-skewness', 'n_[g]-agg-any', 'n_[g]-agg-all', 'n_[g]-agg-mean', 'n_[g]-agg-var', 'n_[g]-agg-min', 'n_[g]-agg-max', 'n_[g]-agg-median', 'n_[g]-agg-sum', 'n_[g]-agg-kurtosis', 'n_[g]-agg-skewness', 'n_[h]-agg-any', 'n_[h]-agg-all', 'n_[h]-agg-mean', 'n_[h]-agg-var', 'n_[h]-agg-min', 'n_[h]-agg-max', 'n_[h]-agg-median', 'n_[h]-agg-sum', 'n_[h]-agg-kurtosis', 'n_[h]-agg-skewness', 'n_[i]-agg-any', 'n_[i]-agg-all', 'n_[i]-agg-mean', 'n_[i]-agg-var', 'n_[i]-agg-min', 'n_[i]-agg-max', 'n_[i]-agg-median', 'n_[i]-agg-sum', 'n_[i]-agg-kurtosis', 'n_[i]-agg-skewness', 'n_[j]-agg-any', 'n_[j]-agg-all', 'n_[j]-agg-mean', 'n_[j]-agg-var', 'n_[j]-agg-min', 'n_[j]-agg-max', 'n_[j]-agg-median', 'n_[j]-agg-sum', 'n_[j]-agg-kurtosis', 'n_[j]-agg-skewness', 'n_[k]-agg-any', 'n_[k]-agg-all', 'n_[k]-agg-mean', 'n_[k]-agg-var', 'n_[k]-agg-min', 'n_[k]-agg-max', 'n_[k]-agg-median', 'n_[k]-agg-sum', 'n_[k]-agg-kurtosis', 'n_[k]-agg-skewness', 'n_[l]-agg-any', 'n_[l]-agg-all', 'n_[l]-agg-mean', 'n_[l]-agg-var', 'n_[l]-agg-min', 'n_[l]-agg-max', 'n_[l]-agg-median', 'n_[l]-agg-sum', 'n_[l]-agg-kurtosis', 'n_[l]-agg-skewness', 'n_[m]-agg-any', 'n_[m]-agg-all', 'n_[m]-agg-mean', 'n_[m]-agg-var', 'n_[m]-agg-min', 'n_[m]-agg-max', 'n_[m]-agg-median', 'n_[m]-agg-sum', 'n_[m]-agg-kurtosis', 'n_[m]-agg-skewness', 'n_[n]-agg-any', 'n_[n]-agg-all', 'n_[n]-agg-mean', 'n_[n]-agg-var', 'n_[n]-agg-min', 'n_[n]-agg-max', 'n_[n]-agg-median', 'n_[n]-agg-sum', 'n_[n]-agg-kurtosis', 'n_[n]-agg-skewness', 'n_[o]-agg-any', 'n_[o]-agg-all', 'n_[o]-agg-mean', 'n_[o]-agg-var', 'n_[o]-agg-min', 'n_[o]-agg-max', 'n_[o]-agg-median', 'n_[o]-agg-sum', 'n_[o]-agg-kurtosis', 'n_[o]-agg-skewness', 'n_[p]-agg-any', 'n_[p]-agg-all', 'n_[p]-agg-mean', 'n_[p]-agg-var', 'n_[p]-agg-min', 'n_[p]-agg-max', 'n_[p]-agg-median', 'n_[p]-agg-sum', 'n_[p]-agg-kurtosis', 'n_[p]-agg-skewness', 'n_[q]-agg-any', 'n_[q]-agg-all', 'n_[q]-agg-mean', 'n_[q]-agg-var', 'n_[q]-agg-min', 'n_[q]-agg-max', 'n_[q]-agg-median', 'n_[q]-agg-sum', 'n_[q]-agg-kurtosis', 'n_[q]-agg-skewness', 'n_[r]-agg-any', 'n_[r]-agg-all', 'n_[r]-agg-mean', 'n_[r]-agg-var', 'n_[r]-agg-min', 'n_[r]-agg-max', 'n_[r]-agg-median', 'n_[r]-agg-sum', 'n_[r]-agg-kurtosis', 'n_[r]-agg-skewness', 'n_[s]-agg-any', 'n_[s]-agg-all', 'n_[s]-agg-mean', 'n_[s]-agg-var', 'n_[s]-agg-min', 'n_[s]-agg-max', 'n_[s]-agg-median', 'n_[s]-agg-sum', 'n_[s]-agg-kurtosis', 'n_[s]-agg-skewness', 'n_[t]-agg-any', 'n_[t]-agg-all', 'n_[t]-agg-mean', 'n_[t]-agg-var', 'n_[t]-agg-min', 'n_[t]-agg-max', 'n_[t]-agg-median', 'n_[t]-agg-sum', 'n_[t]-agg-kurtosis', 'n_[t]-agg-skewness', 'n_[u]-agg-any', 'n_[u]-agg-all', 'n_[u]-agg-mean', 'n_[u]-agg-var', 'n_[u]-agg-min', 'n_[u]-agg-max', 'n_[u]-agg-median', 'n_[u]-agg-sum', 'n_[u]-agg-kurtosis', 'n_[u]-agg-skewness', 'n_[v]-agg-any', 'n_[v]-agg-all', 'n_[v]-agg-mean', 'n_[v]-agg-var', 'n_[v]-agg-min', 'n_[v]-agg-max', 'n_[v]-agg-median', 'n_[v]-agg-sum', 'n_[v]-agg-kurtosis', 'n_[v]-agg-skewness', 'n_[w]-agg-any', 'n_[w]-agg-all', 'n_[w]-agg-mean', 'n_[w]-agg-var', 'n_[w]-agg-min', 'n_[w]-agg-max', 'n_[w]-agg-median', 'n_[w]-agg-sum', 'n_[w]-agg-kurtosis', 'n_[w]-agg-skewness', 'n_[x]-agg-any', 'n_[x]-agg-all', 'n_[x]-agg-mean', 'n_[x]-agg-var', 'n_[x]-agg-min', 'n_[x]-agg-max', 'n_[x]-agg-median', 'n_[x]-agg-sum', 'n_[x]-agg-kurtosis', 'n_[x]-agg-skewness', 'n_[y]-agg-any', 'n_[y]-agg-all', 'n_[y]-agg-mean', 'n_[y]-agg-var', 'n_[y]-agg-min', 'n_[y]-agg-max', 'n_[y]-agg-median', 'n_[y]-agg-sum', 'n_[y]-agg-kurtosis', 'n_[y]-agg-skewness', 'n_[z]-agg-any', 'n_[z]-agg-all', 'n_[z]-agg-mean', 'n_[z]-agg-var', 'n_[z]-agg-min', 'n_[z]-agg-max', 'n_[z]-agg-median', 'n_[z]-agg-sum', 'n_[z]-agg-kurtosis', 'n_[z]-agg-skewness', 'n_[A]-agg-any', 'n_[A]-agg-all', 'n_[A]-agg-mean', 'n_[A]-agg-var', 'n_[A]-agg-min', 'n_[A]-agg-max', 'n_[A]-agg-median', 'n_[A]-agg-sum', 'n_[A]-agg-kurtosis', 'n_[A]-agg-skewness', 'n_[B]-agg-any', 'n_[B]-agg-all', 'n_[B]-agg-mean', 'n_[B]-agg-var', 'n_[B]-agg-min', 'n_[B]-agg-max', 'n_[B]-agg-median', 'n_[B]-agg-sum', 'n_[B]-agg-kurtosis', 'n_[B]-agg-skewness', 'n_[C]-agg-any', 'n_[C]-agg-all', 'n_[C]-agg-mean', 'n_[C]-agg-var', 'n_[C]-agg-min', 'n_[C]-agg-max', 'n_[C]-agg-median', 'n_[C]-agg-sum', 'n_[C]-agg-kurtosis', 'n_[C]-agg-skewness', 'n_[D]-agg-any', 'n_[D]-agg-all', 'n_[D]-agg-mean', 'n_[D]-agg-var', 'n_[D]-agg-min', 'n_[D]-agg-max', 'n_[D]-agg-median', 'n_[D]-agg-sum', 'n_[D]-agg-kurtosis', 'n_[D]-agg-skewness', 'n_[E]-agg-any', 'n_[E]-agg-all', 'n_[E]-agg-mean', 'n_[E]-agg-var', 'n_[E]-agg-min', 'n_[E]-agg-max', 'n_[E]-agg-median', 'n_[E]-agg-sum', 'n_[E]-agg-kurtosis', 'n_[E]-agg-skewness', 'n_[F]-agg-any', 'n_[F]-agg-all', 'n_[F]-agg-mean', 'n_[F]-agg-var', 'n_[F]-agg-min', 'n_[F]-agg-max', 'n_[F]-agg-median', 'n_[F]-agg-sum', 'n_[F]-agg-kurtosis', 'n_[F]-agg-skewness', 'n_[G]-agg-any', 'n_[G]-agg-all', 'n_[G]-agg-mean', 'n_[G]-agg-var', 'n_[G]-agg-min', 'n_[G]-agg-max', 'n_[G]-agg-median', 'n_[G]-agg-sum', 'n_[G]-agg-kurtosis', 'n_[G]-agg-skewness', 'n_[H]-agg-any', 'n_[H]-agg-all', 'n_[H]-agg-mean', 'n_[H]-agg-var', 'n_[H]-agg-min', 'n_[H]-agg-max', 'n_[H]-agg-median', 'n_[H]-agg-sum', 'n_[H]-agg-kurtosis', 'n_[H]-agg-skewness', 'n_[I]-agg-any', 'n_[I]-agg-all', 'n_[I]-agg-mean', 'n_[I]-agg-var', 'n_[I]-agg-min', 'n_[I]-agg-max', 'n_[I]-agg-median', 'n_[I]-agg-sum', 'n_[I]-agg-kurtosis', 'n_[I]-agg-skewness', 'n_[J]-agg-any', 'n_[J]-agg-all', 'n_[J]-agg-mean', 'n_[J]-agg-var', 'n_[J]-agg-min', 'n_[J]-agg-max', 'n_[J]-agg-median', 'n_[J]-agg-sum', 'n_[J]-agg-kurtosis', 'n_[J]-agg-skewness', 'n_[K]-agg-any', 'n_[K]-agg-all', 'n_[K]-agg-mean', 'n_[K]-agg-var', 'n_[K]-agg-min', 'n_[K]-agg-max', 'n_[K]-agg-median', 'n_[K]-agg-sum', 'n_[K]-agg-kurtosis', 'n_[K]-agg-skewness', 'n_[L]-agg-any', 'n_[L]-agg-all', 'n_[L]-agg-mean', 'n_[L]-agg-var', 'n_[L]-agg-min', 'n_[L]-agg-max', 'n_[L]-agg-median', 'n_[L]-agg-sum', 'n_[L]-agg-kurtosis', 'n_[L]-agg-skewness', 'n_[M]-agg-any', 'n_[M]-agg-all', 'n_[M]-agg-mean', 'n_[M]-agg-var', 'n_[M]-agg-min', 'n_[M]-agg-max', 'n_[M]-agg-median', 'n_[M]-agg-sum', 'n_[M]-agg-kurtosis', 'n_[M]-agg-skewness', 'n_[N]-agg-any', 'n_[N]-agg-all', 'n_[N]-agg-mean', 'n_[N]-agg-var', 'n_[N]-agg-min', 'n_[N]-agg-max', 'n_[N]-agg-median', 'n_[N]-agg-sum', 'n_[N]-agg-kurtosis', 'n_[N]-agg-skewness', 'n_[O]-agg-any', 'n_[O]-agg-all', 'n_[O]-agg-mean', 'n_[O]-agg-var', 'n_[O]-agg-min', 'n_[O]-agg-max', 'n_[O]-agg-median', 'n_[O]-agg-sum', 'n_[O]-agg-kurtosis', 'n_[O]-agg-skewness', 'n_[P]-agg-any', 'n_[P]-agg-all', 'n_[P]-agg-mean', 'n_[P]-agg-var', 'n_[P]-agg-min', 'n_[P]-agg-max', 'n_[P]-agg-median', 'n_[P]-agg-sum', 'n_[P]-agg-kurtosis', 'n_[P]-agg-skewness', 'n_[Q]-agg-any', 'n_[Q]-agg-all', 'n_[Q]-agg-mean', 'n_[Q]-agg-var', 'n_[Q]-agg-min', 'n_[Q]-agg-max', 'n_[Q]-agg-median', 'n_[Q]-agg-sum', 'n_[Q]-agg-kurtosis', 'n_[Q]-agg-skewness', 'n_[R]-agg-any', 'n_[R]-agg-all', 'n_[R]-agg-mean', 'n_[R]-agg-var', 'n_[R]-agg-min', 'n_[R]-agg-max', 'n_[R]-agg-median', 'n_[R]-agg-sum', 'n_[R]-agg-kurtosis', 'n_[R]-agg-skewness', 'n_[S]-agg-any', 'n_[S]-agg-all', 'n_[S]-agg-mean', 'n_[S]-agg-var', 'n_[S]-agg-min', 'n_[S]-agg-max', 'n_[S]-agg-median', 'n_[S]-agg-sum', 'n_[S]-agg-kurtosis', 'n_[S]-agg-skewness', 'n_[T]-agg-any', 'n_[T]-agg-all', 'n_[T]-agg-mean', 'n_[T]-agg-var', 'n_[T]-agg-min', 'n_[T]-agg-max', 'n_[T]-agg-median', 'n_[T]-agg-sum', 'n_[T]-agg-kurtosis', 'n_[T]-agg-skewness', 'n_[U]-agg-any', 'n_[U]-agg-all', 'n_[U]-agg-mean', 'n_[U]-agg-var', 'n_[U]-agg-min', 'n_[U]-agg-max', 'n_[U]-agg-median', 'n_[U]-agg-sum', 'n_[U]-agg-kurtosis', 'n_[U]-agg-skewness', 'n_[V]-agg-any', 'n_[V]-agg-all', 'n_[V]-agg-mean', 'n_[V]-agg-var', 'n_[V]-agg-min', 'n_[V]-agg-max', 'n_[V]-agg-median', 'n_[V]-agg-sum', 'n_[V]-agg-kurtosis', 'n_[V]-agg-skewness', 'n_[W]-agg-any', 'n_[W]-agg-all', 'n_[W]-agg-mean', 'n_[W]-agg-var', 'n_[W]-agg-min', 'n_[W]-agg-max', 'n_[W]-agg-median', 'n_[W]-agg-sum', 'n_[W]-agg-kurtosis', 'n_[W]-agg-skewness', 'n_[X]-agg-any', 'n_[X]-agg-all', 'n_[X]-agg-mean', 'n_[X]-agg-var', 'n_[X]-agg-min', 'n_[X]-agg-max', 'n_[X]-agg-median', 'n_[X]-agg-sum', 'n_[X]-agg-kurtosis', 'n_[X]-agg-skewness', 'n_[Y]-agg-any', 'n_[Y]-agg-all', 'n_[Y]-agg-mean', 'n_[Y]-agg-var', 'n_[Y]-agg-min', 'n_[Y]-agg-max', 'n_[Y]-agg-median', 'n_[Y]-agg-sum', 'n_[Y]-agg-kurtosis', 'n_[Y]-agg-skewness', 'n_[Z]-agg-any', 'n_[Z]-agg-all', 'n_[Z]-agg-mean', 'n_[Z]-agg-var', 'n_[Z]-agg-min', 'n_[Z]-agg-max', 'n_[Z]-agg-median', 'n_[Z]-agg-sum', 'n_[Z]-agg-kurtosis', 'n_[Z]-agg-skewness', 'n_[!]-agg-any', 'n_[!]-agg-all', 'n_[!]-agg-mean', 'n_[!]-agg-var', 'n_[!]-agg-min', 'n_[!]-agg-max', 'n_[!]-agg-median', 'n_[!]-agg-sum', 'n_[!]-agg-kurtosis', 'n_[!]-agg-skewness', 'n_[\"]-agg-any', 'n_[\"]-agg-all', 'n_[\"]-agg-mean', 'n_[\"]-agg-var', 'n_[\"]-agg-min', 'n_[\"]-agg-max', 'n_[\"]-agg-median', 'n_[\"]-agg-sum', 'n_[\"]-agg-kurtosis', 'n_[\"]-agg-skewness', 'n_[#]-agg-any', 'n_[#]-agg-all', 'n_[#]-agg-mean', 'n_[#]-agg-var', 'n_[#]-agg-min', 'n_[#]-agg-max', 'n_[#]-agg-median', 'n_[#]-agg-sum', 'n_[#]-agg-kurtosis', 'n_[#]-agg-skewness', 'n_[$]-agg-any', 'n_[$]-agg-all', 'n_[$]-agg-mean', 'n_[$]-agg-var', 'n_[$]-agg-min', 'n_[$]-agg-max', 'n_[$]-agg-median', 'n_[$]-agg-sum', 'n_[$]-agg-kurtosis', 'n_[$]-agg-skewness', 'n_[%]-agg-any', 'n_[%]-agg-all', 'n_[%]-agg-mean', 'n_[%]-agg-var', 'n_[%]-agg-min', 'n_[%]-agg-max', 'n_[%]-agg-median', 'n_[%]-agg-sum', 'n_[%]-agg-kurtosis', 'n_[%]-agg-skewness', 'n_[&]-agg-any', 'n_[&]-agg-all', 'n_[&]-agg-mean', 'n_[&]-agg-var', 'n_[&]-agg-min', 'n_[&]-agg-max', 'n_[&]-agg-median', 'n_[&]-agg-sum', 'n_[&]-agg-kurtosis', 'n_[&]-agg-skewness', \"n_[']-agg-any\", \"n_[']-agg-all\", \"n_[']-agg-mean\", \"n_[']-agg-var\", \"n_[']-agg-min\", \"n_[']-agg-max\", \"n_[']-agg-median\", \"n_[']-agg-sum\", \"n_[']-agg-kurtosis\", \"n_[']-agg-skewness\", 'n_[(]-agg-any', 'n_[(]-agg-all', 'n_[(]-agg-mean', 'n_[(]-agg-var', 'n_[(]-agg-min', 'n_[(]-agg-max', 'n_[(]-agg-median', 'n_[(]-agg-sum', 'n_[(]-agg-kurtosis', 'n_[(]-agg-skewness', 'n_[)]-agg-any', 'n_[)]-agg-all', 'n_[)]-agg-mean', 'n_[)]-agg-var', 'n_[)]-agg-min', 'n_[)]-agg-max', 'n_[)]-agg-median', 'n_[)]-agg-sum', 'n_[)]-agg-kurtosis', 'n_[)]-agg-skewness', 'n_[*]-agg-any', 'n_[*]-agg-all', 'n_[*]-agg-mean', 'n_[*]-agg-var', 'n_[*]-agg-min', 'n_[*]-agg-max', 'n_[*]-agg-median', 'n_[*]-agg-sum', 'n_[*]-agg-kurtosis', 'n_[*]-agg-skewness', 'n_[+]-agg-any', 'n_[+]-agg-all', 'n_[+]-agg-mean', 'n_[+]-agg-var', 'n_[+]-agg-min', 'n_[+]-agg-max', 'n_[+]-agg-median', 'n_[+]-agg-sum', 'n_[+]-agg-kurtosis', 'n_[+]-agg-skewness', 'n_[,]-agg-any', 'n_[,]-agg-all', 'n_[,]-agg-mean', 'n_[,]-agg-var', 'n_[,]-agg-min', 'n_[,]-agg-max', 'n_[,]-agg-median', 'n_[,]-agg-sum', 'n_[,]-agg-kurtosis', 'n_[,]-agg-skewness', 'n_[-]-agg-any', 'n_[-]-agg-all', 'n_[-]-agg-mean', 'n_[-]-agg-var', 'n_[-]-agg-min', 'n_[-]-agg-max', 'n_[-]-agg-median', 'n_[-]-agg-sum', 'n_[-]-agg-kurtosis', 'n_[-]-agg-skewness', 'n_[.]-agg-any', 'n_[.]-agg-all', 'n_[.]-agg-mean', 'n_[.]-agg-var', 'n_[.]-agg-min', 'n_[.]-agg-max', 'n_[.]-agg-median', 'n_[.]-agg-sum', 'n_[.]-agg-kurtosis', 'n_[.]-agg-skewness', 'n_[/]-agg-any', 'n_[/]-agg-all', 'n_[/]-agg-mean', 'n_[/]-agg-var', 'n_[/]-agg-min', 'n_[/]-agg-max', 'n_[/]-agg-median', 'n_[/]-agg-sum', 'n_[/]-agg-kurtosis', 'n_[/]-agg-skewness', 'n_[:]-agg-any', 'n_[:]-agg-all', 'n_[:]-agg-mean', 'n_[:]-agg-var', 'n_[:]-agg-min', 'n_[:]-agg-max', 'n_[:]-agg-median', 'n_[:]-agg-sum', 'n_[:]-agg-kurtosis', 'n_[:]-agg-skewness', 'n_[;]-agg-any', 'n_[;]-agg-all', 'n_[;]-agg-mean', 'n_[;]-agg-var', 'n_[;]-agg-min', 'n_[;]-agg-max', 'n_[;]-agg-median', 'n_[;]-agg-sum', 'n_[;]-agg-kurtosis', 'n_[;]-agg-skewness', 'n_[<]-agg-any', 'n_[<]-agg-all', 'n_[<]-agg-mean', 'n_[<]-agg-var', 'n_[<]-agg-min', 'n_[<]-agg-max', 'n_[<]-agg-median', 'n_[<]-agg-sum', 'n_[<]-agg-kurtosis', 'n_[<]-agg-skewness', 'n_[=]-agg-any', 'n_[=]-agg-all', 'n_[=]-agg-mean', 'n_[=]-agg-var', 'n_[=]-agg-min', 'n_[=]-agg-max', 'n_[=]-agg-median', 'n_[=]-agg-sum', 'n_[=]-agg-kurtosis', 'n_[=]-agg-skewness', 'n_[>]-agg-any', 'n_[>]-agg-all', 'n_[>]-agg-mean', 'n_[>]-agg-var', 'n_[>]-agg-min', 'n_[>]-agg-max', 'n_[>]-agg-median', 'n_[>]-agg-sum', 'n_[>]-agg-kurtosis', 'n_[>]-agg-skewness', 'n_[?]-agg-any', 'n_[?]-agg-all', 'n_[?]-agg-mean', 'n_[?]-agg-var', 'n_[?]-agg-min', 'n_[?]-agg-max', 'n_[?]-agg-median', 'n_[?]-agg-sum', 'n_[?]-agg-kurtosis', 'n_[?]-agg-skewness', 'n_[@]-agg-any', 'n_[@]-agg-all', 'n_[@]-agg-mean', 'n_[@]-agg-var', 'n_[@]-agg-min', 'n_[@]-agg-max', 'n_[@]-agg-median', 'n_[@]-agg-sum', 'n_[@]-agg-kurtosis', 'n_[@]-agg-skewness', 'n_[[]-agg-any', 'n_[[]-agg-all', 'n_[[]-agg-mean', 'n_[[]-agg-var', 'n_[[]-agg-min', 'n_[[]-agg-max', 'n_[[]-agg-median', 'n_[[]-agg-sum', 'n_[[]-agg-kurtosis', 'n_[[]-agg-skewness', 'n_[]]-agg-any', 'n_[]]-agg-all', 'n_[]]-agg-mean', 'n_[]]-agg-var', 'n_[]]-agg-min', 'n_[]]-agg-max', 'n_[]]-agg-median', 'n_[]]-agg-sum', 'n_[]]-agg-kurtosis', 'n_[]]-agg-skewness', 'n_[_]-agg-any', 'n_[_]-agg-all', 'n_[_]-agg-mean', 'n_[_]-agg-var', 'n_[_]-agg-min', 'n_[_]-agg-max', 'n_[_]-agg-median', 'n_[_]-agg-sum', 'n_[_]-agg-kurtosis', 'n_[_]-agg-skewness', 'n_[`]-agg-any', 'n_[`]-agg-all', 'n_[`]-agg-mean', 'n_[`]-agg-var', 'n_[`]-agg-min', 'n_[`]-agg-max', 'n_[`]-agg-median', 'n_[`]-agg-sum', 'n_[`]-agg-kurtosis', 'n_[`]-agg-skewness', 'n_[{]-agg-any', 'n_[{]-agg-all', 'n_[{]-agg-mean', 'n_[{]-agg-var', 'n_[{]-agg-min', 'n_[{]-agg-max', 'n_[{]-agg-median', 'n_[{]-agg-sum', 'n_[{]-agg-kurtosis', 'n_[{]-agg-skewness', 'n_[|]-agg-any', 'n_[|]-agg-all', 'n_[|]-agg-mean', 'n_[|]-agg-var', 'n_[|]-agg-min', 'n_[|]-agg-max', 'n_[|]-agg-median', 'n_[|]-agg-sum', 'n_[|]-agg-kurtosis', 'n_[|]-agg-skewness', 'n_[}]-agg-any', 'n_[}]-agg-all', 'n_[}]-agg-mean', 'n_[}]-agg-var', 'n_[}]-agg-min', 'n_[}]-agg-max', 'n_[}]-agg-median', 'n_[}]-agg-sum', 'n_[}]-agg-kurtosis', 'n_[}]-agg-skewness', 'n_[~]-agg-any', 'n_[~]-agg-all', 'n_[~]-agg-mean', 'n_[~]-agg-var', 'n_[~]-agg-min', 'n_[~]-agg-max', 'n_[~]-agg-median', 'n_[~]-agg-sum', 'n_[~]-agg-kurtosis', 'n_[~]-agg-skewness', 'n_[ ]-agg-any', 'n_[ ]-agg-all', 'n_[ ]-agg-mean', 'n_[ ]-agg-var', 'n_[ ]-agg-min', 'n_[ ]-agg-max', 'n_[ ]-agg-median', 'n_[ ]-agg-sum', 'n_[ ]-agg-kurtosis', 'n_[ ]-agg-skewness', 'n_[\\x0c]-agg-any', 'n_[\\x0c]-agg-all', 'n_[\\x0c]-agg-mean', 'n_[\\x0c]-agg-var', 'n_[\\x0c]-agg-min', 'n_[\\x0c]-agg-max', 'n_[\\x0c]-agg-median', 'n_[\\x0c]-agg-sum', 'n_[\\x0c]-agg-kurtosis', 'n_[\\x0c]-agg-skewness', 'n_[\\\\]-agg-any', 'n_[\\\\]-agg-all', 'n_[\\\\]-agg-mean', 'n_[\\\\]-agg-var', 'n_[\\\\]-agg-min', 'n_[\\\\]-agg-max', 'n_[\\\\]-agg-median', 'n_[\\\\]-agg-sum', 'n_[\\\\]-agg-kurtosis', 'n_[\\\\]-agg-skewness', 'n_[^]-agg-any', 'n_[^]-agg-all', 'n_[^]-agg-mean', 'n_[^]-agg-var', 'n_[^]-agg-min', 'n_[^]-agg-max', 'n_[^]-agg-median', 'n_[^]-agg-sum', 'n_[^]-agg-kurtosis', 'n_[^]-agg-skewness']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sherlock.deploy.train_sherlock import train_sherlock\n",
    "from sherlock.deploy.predict_sherlock import predict_sherlock_proba, _transform_predictions_to_classes\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "predict_sherlock_proba = predict_sherlock_proba(X_test, nn_id=nn_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = []\n",
    "\n",
    "for i in range(len(y_test)):\n",
    "    nn_probs = predict_sherlock_proba[i]\n",
    "    voting_probs = predicted_prob[i]\n",
    "    \n",
    "    x = nn_probs + voting_probs\n",
    "    x = x / 2\n",
    "\n",
    "    combined.append(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels = _transform_predictions_to_classes(combined, nn_id=nn_model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction count 137353, type = <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9043127091005166"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'prediction count {len(predicted_labels)}, type = {type(predicted_labels)}')\n",
    "\n",
    "size=len(y_test)\n",
    "\n",
    "# Should be fully deterministic too.\n",
    "f1_score(y_test[:size], predicted_labels[:size], average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.8914784495960828 RandomForestClassifier\n",
    "# 0.8888581433158012 ExtraTreesClassifier\n",
    "# 0.8939142078215914 VotingClassifier\n",
    "\n",
    "# 0.9051046508515834  NN + votingclassifier\n",
    "# 0.9037255679859006 RFC(100) + NN\n",
    "# 0.904082859543776 RFC(300) + NN\n",
    "\n",
    "# 0.905148377678918 NN + VotingClassifier(RFC100 + ETC100)\n",
    "# 0.9057069845572598 NN (retrained_sherlock8) + VotingClassifier(RFC300 + ETC300)\n",
    "\n",
    "# 0.9057005326978261 NN (retrained_sherlock8) + VotingClassifier(RFC100 + ETC100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(y_test, predicted_labels, output_dict=True)\n",
    "\n",
    "class_scores = list(filter(lambda x: isinstance(x, tuple) and isinstance(x[1], dict) and 'f1-score' in x[1] and x[0] in classes, list(report.items())))\n",
    "\n",
    "class_scores = sorted(class_scores, key=lambda item: item[1]['f1-score'], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 5 Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tf1-score\tprecision\trecall\t\tsupport\n",
      "grades\t\t0.994\t\t0.991\t\t0.997\t\t1765\n",
      "isbn\t\t0.990\t\t0.990\t\t0.990\t\t1430\n",
      "jockey\t\t0.988\t\t0.987\t\t0.990\t\t2819\n",
      "industry\t0.984\t\t0.979\t\t0.989\t\t2958\n",
      "currency\t0.978\t\t0.987\t\t0.968\t\t405\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\t\\tf1-score\\tprecision\\trecall\\t\\tsupport\")\n",
    "\n",
    "for key, value in class_scores[0:5]:\n",
    "    if len(key) >= 8:\n",
    "        tabs = '\\t' * 1\n",
    "    else:\n",
    "        tabs = '\\t' * 2\n",
    "\n",
    "    print(f\"{key}{tabs}{value['f1-score']:.3f}\\t\\t{value['precision']:.3f}\\t\\t{value['recall']:.3f}\\t\\t{value['support']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bottom 5 Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tf1-score\tprecision\trecall\t\tsupport\n",
      "rank\t\t0.746\t\t0.708\t\t0.787\t\t2983\n",
      "person\t\t0.690\t\t0.736\t\t0.649\t\t579\n",
      "sales\t\t0.622\t\t0.750\t\t0.531\t\t322\n",
      "director\t0.612\t\t0.719\t\t0.533\t\t225\n",
      "ranking\t\t0.584\t\t0.822\t\t0.453\t\t439\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\t\\tf1-score\\tprecision\\trecall\\t\\tsupport\")\n",
    "\n",
    "for key, value in class_scores[len(class_scores)-5:len(class_scores)]:\n",
    "    if len(key) >= 8:\n",
    "        tabs = '\\t' * 1\n",
    "    else:\n",
    "        tabs = '\\t' * 2\n",
    "\n",
    "    print(f\"{key}{tabs}{value['f1-score']:.3f}\\t\\t{value['precision']:.3f}\\t\\t{value['recall']:.3f}\\t\\t{value['support']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "       address      0.930     0.951     0.940      3003\n",
      "     affiliate      0.976     0.814     0.888       204\n",
      "   affiliation      0.982     0.952     0.967      1768\n",
      "           age      0.884     0.958     0.919      3033\n",
      "         album      0.893     0.886     0.890      3035\n",
      "          area      0.897     0.840     0.868      1987\n",
      "        artist      0.803     0.887     0.843      3043\n",
      "    birth date      0.971     0.975     0.973       479\n",
      "   birth place      0.984     0.911     0.947       418\n",
      "         brand      0.821     0.718     0.766       574\n",
      "      capacity      0.839     0.765     0.801       362\n",
      "      category      0.925     0.898     0.911      3087\n",
      "          city      0.855     0.908     0.880      2966\n",
      "         class      0.908     0.927     0.917      2971\n",
      "classification      0.943     0.869     0.904       587\n",
      "          club      0.978     0.954     0.965      2977\n",
      "          code      0.931     0.925     0.928      2956\n",
      "    collection      0.955     0.937     0.946       476\n",
      "       command      0.924     0.936     0.930      1045\n",
      "       company      0.902     0.897     0.900      3041\n",
      "     component      0.886     0.893     0.890      1226\n",
      "     continent      0.900     0.916     0.908       227\n",
      "       country      0.900     0.944     0.921      3038\n",
      "        county      0.940     0.961     0.951      2959\n",
      "       creator      0.900     0.830     0.864       347\n",
      "        credit      0.876     0.843     0.859       941\n",
      "      currency      0.987     0.968     0.978       405\n",
      "           day      0.947     0.915     0.931      3038\n",
      "         depth      0.940     0.949     0.945       947\n",
      "   description      0.823     0.874     0.848      3042\n",
      "      director      0.719     0.533     0.612       225\n",
      "      duration      0.945     0.952     0.949      3000\n",
      "     education      0.867     0.853     0.860       313\n",
      "     elevation      0.965     0.954     0.959      1299\n",
      "        family      0.970     0.906     0.937       746\n",
      "     file size      0.924     0.870     0.896       361\n",
      "        format      0.973     0.957     0.965      2956\n",
      "        gender      0.851     0.843     0.847      1030\n",
      "         genre      0.939     0.964     0.951      1163\n",
      "        grades      0.991     0.997     0.994      1765\n",
      "      industry      0.979     0.989     0.984      2958\n",
      "          isbn      0.990     0.990     0.990      1430\n",
      "        jockey      0.987     0.990     0.988      2819\n",
      "      language      0.916     0.946     0.931      1474\n",
      "      location      0.903     0.831     0.866      2949\n",
      "  manufacturer      0.867     0.817     0.841       945\n",
      "          name      0.741     0.755     0.748      3017\n",
      "   nationality      0.840     0.757     0.797       424\n",
      "         notes      0.760     0.836     0.797      2303\n",
      "      operator      0.878     0.837     0.857       404\n",
      "         order      0.872     0.874     0.873      1462\n",
      "  organisation      0.863     0.844     0.853       262\n",
      "        origin      0.968     0.905     0.935      1439\n",
      "         owner      0.936     0.877     0.905      1673\n",
      "        person      0.736     0.649     0.690       579\n",
      "         plays      0.823     0.913     0.866      1513\n",
      "      position      0.827     0.862     0.844      3057\n",
      "       product      0.880     0.885     0.883      2647\n",
      "     publisher      0.915     0.918     0.917       880\n",
      "         range      0.927     0.794     0.855       577\n",
      "          rank      0.708     0.787     0.746      2983\n",
      "       ranking      0.822     0.453     0.584       439\n",
      "        region      0.878     0.861     0.869      2740\n",
      "      religion      0.969     0.918     0.943       340\n",
      "   requirement      0.961     0.817     0.883       300\n",
      "        result      0.967     0.942     0.955      2920\n",
      "         sales      0.750     0.531     0.622       322\n",
      "       service      0.967     0.927     0.946      2222\n",
      "           sex      0.944     0.935     0.939      2997\n",
      "       species      0.931     0.950     0.940       819\n",
      "         state      0.951     0.957     0.954      3030\n",
      "        status      0.939     0.943     0.941      3100\n",
      "        symbol      0.955     0.971     0.963      1752\n",
      "          team      0.864     0.870     0.867      3011\n",
      "     team name      0.873     0.849     0.861      1639\n",
      "          type      0.904     0.889     0.896      2909\n",
      "        weight      0.956     0.950     0.953      2963\n",
      "          year      0.972     0.940     0.956      3015\n",
      "\n",
      "      accuracy                          0.904    137353\n",
      "     macro avg      0.903     0.879     0.890    137353\n",
      "  weighted avg      0.906     0.904     0.904    137353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, predicted_labels, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total mismatches: 13132 (F1 score: 0.9043127091005166)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('name', 738),\n",
       " ('rank', 634),\n",
       " ('location', 497),\n",
       " ('position', 422),\n",
       " ('team', 391),\n",
       " ('description', 383),\n",
       " ('region', 382),\n",
       " ('notes', 377),\n",
       " ('album', 345),\n",
       " ('artist', 343),\n",
       " ('type', 324),\n",
       " ('area', 317),\n",
       " ('category', 316),\n",
       " ('company', 312),\n",
       " ('product', 304),\n",
       " ('city', 273),\n",
       " ('day', 258),\n",
       " ('team name', 247),\n",
       " ('ranking', 240),\n",
       " ('code', 221),\n",
       " ('class', 216),\n",
       " ('owner', 206),\n",
       " ('person', 203),\n",
       " ('sex', 196),\n",
       " ('order', 184),\n",
       " ('year', 180),\n",
       " ('status', 176),\n",
       " ('manufacturer', 173),\n",
       " ('country', 171),\n",
       " ('result', 168),\n",
       " ('service', 163),\n",
       " ('brand', 162),\n",
       " ('gender', 162),\n",
       " ('sales', 151),\n",
       " ('weight', 148),\n",
       " ('credit', 148),\n",
       " ('address', 148),\n",
       " ('duration', 144),\n",
       " ('club', 138),\n",
       " ('origin', 137),\n",
       " ('plays', 131),\n",
       " ('component', 131),\n",
       " ('state', 129),\n",
       " ('age', 127),\n",
       " ('format', 127),\n",
       " ('range', 119),\n",
       " ('county', 114),\n",
       " ('director', 105),\n",
       " ('nationality', 103),\n",
       " ('capacity', 85),\n",
       " ('affiliation', 84),\n",
       " ('language', 80),\n",
       " ('classification', 77),\n",
       " ('publisher', 72),\n",
       " ('family', 70),\n",
       " ('command', 67),\n",
       " ('operator', 66),\n",
       " ('elevation', 60),\n",
       " ('creator', 59),\n",
       " ('requirement', 55),\n",
       " ('symbol', 51),\n",
       " ('depth', 48),\n",
       " ('file size', 47),\n",
       " ('education', 46),\n",
       " ('genre', 42),\n",
       " ('organisation', 41),\n",
       " ('species', 41),\n",
       " ('affiliate', 38),\n",
       " ('birth place', 37),\n",
       " ('industry', 32),\n",
       " ('collection', 30),\n",
       " ('religion', 28),\n",
       " ('jockey', 28),\n",
       " ('continent', 19),\n",
       " ('isbn', 14),\n",
       " ('currency', 13),\n",
       " ('birth date', 12),\n",
       " ('grades', 6)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "size = len(y_test)\n",
    "mismatches = list()\n",
    "\n",
    "for idx, k1 in enumerate(y_test[:size]):\n",
    "    k2 = predicted_labels[idx]\n",
    "\n",
    "    if k1 != k2:\n",
    "        mismatches.append(k1)\n",
    "#        if k1 in ('brand'):\n",
    "#        print(f'[{idx}] expected \"{k1}\" but predicted \"{k2}\"')\n",
    "        \n",
    "f1 = f1_score(y_test[:size], predicted_labels[:size], average=\"weighted\")\n",
    "print(f'Total mismatches: {len(mismatches)} (F1 score: {f1})')\n",
    "\n",
    "data = Counter(mismatches)\n",
    "data.most_common()   # Returns all unique items and their counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = pd.read_parquet('../data/raw/test_values.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted \"age\", actual label \"position\". Actual values:\n",
      "[[2, 4]]\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "\n",
    "idx = 541\n",
    "original = test_samples.iloc[idx]\n",
    "converted = original.apply(literal_eval).to_list()\n",
    "\n",
    "print(f'Predicted \"{predicted_labels[idx]}\", actual label \"{y_test[idx]}\". Actual values:\\n{converted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
